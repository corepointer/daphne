// Linear regression model training on random data (double precision).
tg1=now();
// Data generation.
XY = rand($numRows, $numCols, as.f32(0.0), as.f32(1.0), 1, 42);
tg2=now();
print("Data gen time: ", 0);
print((tg2 - tg1)/1000000, 0);
print(" milliseconds", 1);

tp0 = now();

// Extraction of X and y.
X = XY[, seq(0, as.si64($numCols) - 2, 1)];
y = XY[, seq(as.si64($numCols) - 1, as.si64($numCols) - 1, 1)];

// Normalization, standardization.
Xmeans = mean(X, 1);
Xstddev = stddev(X, 1);
// Could be expressed as follows to simplify vectorization:
//Xmeans = sum(X, 1) / as.f64($numRows);
//Xstddev = sqrt(sum((X - Xmeans) ^ 2.0, 1) / as.f64($numRows));
#print(sum(Xmeans));
#print(sum(Xstddev));

X = (X - Xmeans) / Xstddev;
#print(sum(X));
X = cbind(X, fill(as.f32(1.0), nrow(X), 1));
##print(sum(X));
#//A = t(X) @ X;
#// Can be rewritten as follows to simplify vectorization (and for general speed-up):
A = syrk(X);
##print(sum(A));
lambda = fill(as.f32(0.001), ncol(X), 1);
A = A + diagMatrix(lambda);
#print(sum(y));

#b = t(X) @ y;
#// Can be rewritten as follows to simplify vectorization:
#b = t(t(y) @ X);
#// or
#b = t(sum((X * y), 1));
b=gemv(X,y);
beta = solve(A, b);

tp1 = now();
#print(tp1 - tp0, 1, 2);
print("A: "); print(sum(A));
print("b: "); print(sum(b));

print("Execution time: ", 0);
print((tp1 - tp0)/1000000, 0);
print(" milliseconds", 1);

// ****************************************************************************
// Result output
// ****************************************************************************

print("Result sum: ", 0); print(sum(beta));

